# \#1

## 작품 설명 (요약)

```text

```





# \#2

## 개발 개요

> 프로젝트 개발 배경, 동기, 목표, 필요성 등 (2 Page)

### 개발 작품 개요 (제안하는 작품에 대한 개요를 자세히 서술한다)

```
- 농인들의 통화 서비스 이용 접근성 향상을 위해 실시간 통화 동시통역 시스템을 개발하였음.
- 각자의 통화 환경에 맞게 시스템 구축. 서비스 제공자는 음성을 활용하여 대화하고 서비스 대상자는 텍스트와 이모지를 활용해 대화함.
- 서비스 제공자의 음성이 대상자에게 텍스트로 변환되어 전달, 대상자가 작성한 텍스트는 제공자에게 음성으로 변환되어 다시 전달됨. 변환을 담당하는 하드웨어는 라즈베리파이4.
- 
```



### 개발 목표 (개발 목표를 명확하게 제시한다.)

```
    - 음성을 텍스트로 바꾸어주는 STT 기술을 해당 서비스에 맞게 최적화 된 방법으로 개발한다.
    - 텍스트를 음성으로 바꾸어주는 TTS 기술을 해당 서비스에 맞게 개발한다.
    - 텍스트를 음성으로 바꾸는 과정에서 음성에 감정을 표현할 수 있도록 개발한다.
```



### 개발 작품의 필요성 (개발하는 작품이 왜 필요한지에 대해 상세히 서술한다)

```
    - 현재 음성 통화를 텍스트로, 텍스트를 음성으로 실시간으로 바꿔주는 서비스가 제공되지 않고 있음.
    - 수어 보급률이 낮다. 수어를 사용해서 통화가 가능하면 좋겠지만 보통 통화에 영상통화는 제공되지 않는다. 그럼 텍스트로 대화를 해야하는데
    이 부분에서 감정을 표현하고 싶을 수 있다. 이런 부분 부각시키면 좋을듯?
    - 애화학교 근처에 거주하며 자연스레 청각장애인이 많은 환경에서 나고 자라 주변에 청각장애를 앓고 있으신 분들이 많고, 팀장 또한 2016년 부터 돌발성 난청을 진단받아 꾸준히 청력이 떨어지고있어 자연스레 청각장애인을 위한 시스템 개발에 관심을 가지게 되었다. 또한 다양한 유선전화 상담 아르바이트 경험자로서, 주장의 강도가 높지 않거나, 발생 즉시 항의하지 않는다면 컴플레인 처리를 늦추는 프로세스를 가지고있어 농인들은 이런 상담에서조차 알게모르게 차별받는 현실이다. 이를 해결하기위해 어쩌구 저쩌구
    - 텍스트로 의사소통을 진행하는 경우 문자로 대화하기 힘든 상황에서 어려움이 발생할 수 있다. 이 작품에서는 일반 통화에서도 비교적 빠른 의사소통이 가능하다. 또한 감정을 표현할 수 있는 기능을 넣어, 선천적으로 청각장애가있어 발화가 어려운 사람들이 컴플레인 시 감정표현을 할 수 있게도 하였다.
    - 언어장애인들을 위한 TTS 시스템이나 청각장애인 사용자를 위한 문자 이용 안내 통화연결음보다 더 편리하고, 중계사를 통한 문자 읽어주기 서비스 등 청각장애인들을 위한 많은 통화 서비스보다 더 상용화가 쉽다는 데 장점을 둔다.


```





# \#3

## 개발 환경 설명

> 최대한 자세하게 기술 (15 Page)

### Hardware 구성

```
- Raspberry Pi 4
- HiFiberry DAC/ADC I/O


- 1. 통화를 통해 받아온 상대방의 음성을 HiFiBerry의 DAC 포트를 이용해 RPI로 전달한다.
- 2. RPI에서 이를 STT 서버로 전달하고, 그 결과물(Text)를 화면에 띄운다.
- 3. 사용자의 답변(Text)를 입력받은 후 이를 TTS 서버로 전달한다.
- 4. 그 결과물(Speech)를 HifiBerry의 ADC포트를 이용해 휴대폰으로 전달한다. 이는 상대방에게 음성 형태로 전달된다.

```



### Hardware 기능 (제어 방법 등 서술)

```

```



### Software 구성

```
1. 구현 기술
- Kaldi를 활용한 한국어, 배달 및 서비스 제공 특화 STT 구현
- TCP/IP 통신을 활용한 서버/클라이언트 네트워크 구현
- STT 결과 값 후보정을 위한 띄어쓰기 및 rescore 구현
- 텍스트에서 감정을 읽어내는 감정인식기(나중에 더 그럴듯하게 바꾸기) 구현
- 상황에 따라 말투 및 억양을 바꾸는 감정별 TTS 구현
- TKinter를 활용한 User Interface 구현

2. 서버/클라이언트 구축
1) 메인 서버
- STT가 작동하는 메인 서버
2) 서브 서버
- 감정별 TTS가 작동하는 서브 서버
3) 클라이언트
- 띄어쓰기 보정 클라이언트
- rescore 보정 클라이언트
- 감정인식기 클라이언트
- User Interace 클라이언트
- 음성 실시간 수신 클라이언트
- 음성 실시간 송신 클라이언트

표로 정리하면 좋을 듯
```



### Software 설계도 (흐름도 및 클래스 다이어그램 등 / 개발언어에 따라 선택)

```
# 학습 영역
1. VVS STT 학습 구성도(간단한 학습 절차 설명 필요)
2. 감정 TTS 학습 구성도(간단한 학습 절차 설명 필요)
3. 감정인식기 학습 구성도(간단한 학습 절차 설명 필요)
4. 띄어쓰기 보정 학습 구성도(간단한 학습 절차 설명 필요)

# 서버 영역
1. TCP 구성도 넣고 세부적인 설명 필요(순서도 같이 넣어주면 졸을듯)

# Raspberry Pi 영역
1. UI 구성 (인터페이스 사진 같은 것을 첨부해서 설명해주시면 좋을듯)
2. WAV 받기 설계
```



### Software 기능 (필요 시 알고리즘 설명 포함)

```
VVS STT가 어떤 방식으로 작동하는가 위주로 설명해주면 좋을듯
-> 추가 부가설명을 띄어쓰기와 리스코어를 위한 보정도 설명 필요
-> input, output을 구분해서 어떤 것을 받고 어떤 것을 보내주는지 해주는 것도 좋을듯?
- 감정 TTS 작동 방식
- 감정 인식기 작동 방식
- 라베파에서 쏴주는 음성과 받는 음성 클라이언트 작동 방식 설명
```



### 프로그램 사용법 (Interface)

```
- 학습을 위한 파일 리스트들 나열
- UI의 작동 방식과 서버를 키는 방식? 같은 것들을 설명할 필요가 있을듯
```



### 개발환경 (언어, Tool, 사용시스템 등)

```
- shell script, perl script, c++, python
- Tool: tornado, selenium, ws4py, json, tensorflow, pytorch
- 시스템: linux, Raspbian, code-server, Jupyter server

```





# \#4

## 개발 프로그램 설명

> 최대한 자세하게 기술 (8 Page)

### 파일 구성

```

```



### 함수별 기능

```

```



### 주요 함수의 흐름도

```

```



### 기술적 차별성

```

```





# \#5

## 개발 중 발생한 장애요인과 해결방안

>개발 과정에서 나타났던 모든 장애 요인(Risk)들을 나열하고, 이러한 장애요인들이 발생했던 경우 어떻게 해결했는지 구체적으로 제시 

```
○ 
  - 딥러닝 모델로 진행하려 했으나, 실시간 분석 성능이 좋은 모델을 사용하기 어려웠고, 있다해도 한국어 성능이 매우 조악했음 (RNN-T 모델의 경우 잘 나와야 WER 20%)

    - sox 음성코덱 재현 문제
    - 온라인 디코딩 문제 (한국어를 위한 nnet3 model 존재 x)
    - Language Model 추가 (기존 interpolation 방식을 사용하는 것이 아닌 최신 phrase hint 기능을 사용하기 위해 다양한 연구)
	(- 배달 관련 말뭉치 구하는 문제 (selenium을 이용한 웹 크롤링
	 - 말뭉치 하나 학습하는데도 많은 시간이 걸렸음 -> phrase hint
	- 또한 interpolation 하고 나면 손상이 심해 다른 분야에 확장 어렵
	)
그래서 arpa로 합치는데

  ○ 
    - 낮은 computing 성능 (부족한 RAM을 해결하기 위해 parallel을 이용한 병렬 처리 추가 및 arpa파일 쪼개기 수행)
    - kaldi documentation의 빈약한 부분을 각자 스스로 공부하여 채워넣어야했음
(kaldi binary 파일 분석 및 다양한 shell script 파일 분석, kaldi I/O, 문법 파악 등)
    - 

intel mkl 동적 라이브러리 설정 오류,

  ○ 
    - ㅈㄴㄴㄴㄴㄴ방대한 시스템 파일간의 구조파악이 어려웠음 

메모리 초과로 인한 문제 발생
대용량의 데이터를 다루는 만큼 gz을 이용한 문장 처리 시, 순간적으로 메모리 오버플로우가 일어나는데, 이를 알아차리기 전까지는
문법이 잘못되었다던가, 파이프가 고장났다는 둥의 실제 문제와는 관련없는 LOG가 남아 문제해결을 하는데 많은 어려움을 겪었습니다

HW
1. AV Jack과 사운드카드를 이용해 시도
Raspberry Pi4에 마이크 입력회로가 연결되어 있지 않아 3.5mm Jack으로 소리의 입력을 받아오지 못함
2. bluetooth(input) + AV Jack(output)
휴대폰 통화 상에서 bluetooth mic와 일반 케이블 외부마이크가 동시작동이 되지 않음.
Raspberry pi의 연결이 원활하지 않아 bluetooth 동글 4.0 으로 진행했으나 자꾸 끊기는 문제는 지속적으로 발생함.
이에 블루투스 환경을 피해 최대한 하드웨어적으로 구현할 수 있도록 DAC와 ADC 모듈이 탑재된 HiFiBerry 카드를 이용함으로서 해결할 수 있었음.


```







# \#6

## 개발결과물의 차별성

>개발한 결과물과 기존 발표된 유사작품(제품) 간 차별성 및 우수성 설명 (1Page)

```
  ○ API보다 이게 더 나은 이유
    - 비용이 들지 않음 (실시간, 오프라인 구현)
    - 우리가 사용하는 domain에 특화 가능
    - 다양한 상용품에 준하는 성능(zeroth ee를 사용하는 기업 list 참조)
    - 한국어 관련 Open source가 없는 kaldi tool을 이용

  ○ 최신 DEEP LEARNING 보다 나은 이 유
    - 최신 deep learning model을 사용하였을땐
    - (facebook의 wav2vec2.0을 활용한 PORORO의 asr보다 우수한 인식능력을 보임)
    - 또한 최신 모델에서는 실시간 음성인식이 어려운 문제를 해결하지 못함

  ○ Hear World Communication - Captioned Speech-to-Text Telephones (2011)
    - 영문만 지원
    - 유선전화를 통해 STT만 텍스트로 보여줌
    - 이건 한국어를 지원
    - 청각장애인의 TTS를 통해 양방향 통신이 가능
    - 유선전화를 이용하지 않더라도 휴대폰으로도 충분히 이용 가능


```





# \#7

## 개발 일정

> 실제 프로젝트 개발 일정 작성 (1Page)

```
- 인공지능 공부
- 다양한 NLP 모델 학습 (참고한 논문 및 github 첨부)
- 
- STT, TTS 관련하여 학습

- KALDI 구조 (wFST(HCLG) 구조) 파악
- AM, LM train 방법 파악 (arpa 파일 구조 파악, 등등,)
- 한국어 음운 특징 파악
- fst 제작 방식 파악 openfst 분석  
- decoding 방식 파악
- lattice 구조 파악
- 코덱 분석해서 최대한 비슷하게 만드는 작업

```

